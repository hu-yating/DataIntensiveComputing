{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b49d6fdb-5158-40af-a32c-91edd90032bc",
   "metadata": {},
   "source": [
    "# Lab2 - Spark Streaming and GraphX\n",
    "\n",
    "Group 2 Ya Ting Hu & Zhen Tian"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28c7b013-c3b3-47ae-9f31-5d965adb566c",
   "metadata": {},
   "source": [
    "## **Task 1**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ee05b79-020c-40b2-8385-01e541b7f028",
   "metadata": {},
   "source": [
    "#### build.sbt\n",
    "```scala\n",
    "name := \"spark_kafka\"\n",
    "\n",
    "version := \"1.0\"\n",
    "\n",
    "scalaVersion := \"2.11.8\"\n",
    "\n",
    "libraryDependencies ++= Seq(\n",
    "  \"org.apache.spark\" %% \"spark-core\" % \"2.2.1\",\n",
    "  \"org.apache.spark\" %% \"spark-sql\" % \"2.2.1\",\n",
    "  \"org.apache.spark\" % \"spark-streaming_2.11\" % \"2.2.1\",\n",
    "  \"org.apache.spark\" % \"spark-streaming-kafka-0-8_2.11\" % \"2.2.1\",\n",
    "  (\"com.datastax.spark\" %% \"spark-cassandra-connector\" % \"2.0.2\").exclude(\"io.netty\", \"netty-handler\"),\n",
    "  (\"com.datastax.cassandra\" % \"cassandra-driver-core\" % \"3.0.0\").exclude(\"io.netty\", \"netty-handler\")\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cd65c2a-c694-4c50-9e2f-b40642c90103",
   "metadata": {},
   "source": [
    "#### KafkaSpark.scala\n",
    "\n",
    "In this assignment, first we import the given packages. In the main function, first we make a manual spark streaming context and use `createDirectStream` to connect to Kafka, with topic `avg`. Second, we extract the key value pair and split it, cast the value to `Double` type. Then we make the mapping function. To get the count and sum values, we modified the `state` and `return` from the template code. In the main function, we let the count += 1 and add up the state value and new value to make the summation. With the new variable count, it is possible to compute the average using summation divided by count. In the end we shall update the state with new count and summation and output the key, average pair.\n",
    "\n",
    "```scala\n",
    "package sparkstreaming\n",
    "\n",
    "import java.util.HashMap\n",
    "import org.apache.kafka.clients.consumer.ConsumerConfig\n",
    "import org.apache.kafka.common.serialization.StringDeserializer\n",
    "import org.apache.spark.streaming.kafka._\n",
    "import kafka.serializer.{DefaultDecoder, StringDecoder}\n",
    "import org.apache.spark.SparkConf\n",
    "import org.apache.spark.streaming._\n",
    "import org.apache.spark.streaming.kafka._\n",
    "import org.apache.spark.storage.StorageLevel\n",
    "import java.util.{Date, Properties}\n",
    "import org.apache.kafka.clients.producer.{KafkaProducer, ProducerRecord, ProducerConfig}\n",
    "import scala.util.Random\n",
    "\n",
    "\n",
    "object KafkaSpark {\n",
    "  def main(args: Array[String]) {\n",
    "    val kafkaParams = Map(\n",
    "    \"metadata.broker.list\" -> \"localhost:9092\",\n",
    "    \"zookeeper.connect\" -> \"localhost:2181\",\n",
    "    \"group.id\" -> \"kafka-spark-streaming\",\n",
    "    \"zookeeper.connection.timeout.ms\" -> \"1000\")\n",
    "\n",
    "    // make a connection to Kafka and read (key, value) pairs from it\n",
    "    val sparkConf = new SparkConf().setMaster(\"local[2]\").setAppName(\"readKeyValuePairFromKafka\")\n",
    "    val ssc = new StreamingContext(sparkConf, Seconds(1))\n",
    "   \n",
    "    ssc.checkpoint(\".\")\n",
    "    \n",
    "    val topic = Set(\"avg\")\n",
    "    val messages = KafkaUtils.createDirectStream[String, String, StringDecoder,StringDecoder](\n",
    "      ssc,\n",
    "      kafkaParams,\n",
    "      topic\n",
    "    )\n",
    "\n",
    "    val kvpair_str = messages.map(_._2).map(_.split(\",\"))\n",
    "    val kvpair_float = kvpair_str.map(x => (x(0), x(1).toDouble))\n",
    "    \n",
    "    def mappingFunc(key: String, value: Option[Double], state: State[(Int, Double)]): (String, Double) = {\n",
    "      val value_new = value.getOrElse(0.0);\n",
    "      var (count_state, sum_state) = state.getOption.getOrElse((0, 0.0))\n",
    "\n",
    "      count_state   = count_state + 1;\n",
    "      sum_state     = sum_state   + value_new;\n",
    "\n",
    "      val avg_state = sum_state/count_state;\n",
    "\n",
    "      state.update((count_state, sum_state))\n",
    "      (key,avg_state)\n",
    "    }\n",
    "\n",
    "    val stateDstream = kvpair_float.mapWithState(StateSpec.function(mappingFunc _))\n",
    "\n",
    "    \n",
    "    stateDstream.print()\n",
    "    ssc.start()\n",
    "    ssc.awaitTermination()\n",
    "  }\n",
    "}\n",
    "```\n",
    "\n",
    "#### **Output:**\n",
    "```bash\n",
    "-------------------------------------------\n",
    "Time: 1633819471000 ms\n",
    "-------------------------------------------\n",
    "(x,12.578697929658269)\n",
    "(p,12.71021135727018)\n",
    "(j,12.463486140724946)\n",
    "(j,12.46656008526512)\n",
    "(n,12.668130165289256)\n",
    "(d,12.524929685502428)\n",
    "(l,12.581098339719029)\n",
    "(h,12.483510908168443)\n",
    "(b,12.468790468790468)\n",
    "(v,12.48776637726914)\n",
    "...\n",
    "\n",
    "21/10/09 18:44:33 INFO JobScheduler: Finished job streaming job 1633819471000 ms.0 from job set of time 1633819471000 ms\n",
    "21/10/09 18:44:33 INFO JobScheduler: Total delay: 2.862 s for time 1633819471000 ms (execution: 0.712 s)\n",
    "21/10/09 18:44:33 INFO MapPartitionsRDD: Removing RDD 12 from persistence list\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "177578b6-7474-4f52-b38d-b3e397c6cd30",
   "metadata": {},
   "source": [
    "## **Task 2**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98faa76a-9aa6-45fc-8703-a4631435fdcc",
   "metadata": {},
   "source": [
    "#### build.sbt\n",
    "\n",
    "Added given packages.\n",
    "\n",
    "```scala\n",
    "name := \"spark_kafka\"\n",
    "\n",
    "version := \"1.0\"\n",
    "\n",
    "scalaVersion := \"2.11.8\"\n",
    "\n",
    "libraryDependencies ++= Seq(\n",
    "  \"org.apache.spark\" %% \"spark-core\" % \"2.2.1\",\n",
    "  \"org.apache.spark\" %% \"spark-sql\" % \"2.2.1\",\n",
    "  \"org.apache.spark\" % \"spark-streaming_2.11\" % \"2.2.1\",\n",
    "  \"org.apache.spark\" % \"spark-sql-kafka-0-10_2.11\" % \"2.2.1\",\n",
    "  \"org.apache.spark\" % \"spark-streaming-kafka-0-8_2.11\" % \"2.2.1\",\n",
    "  (\"com.datastax.spark\" %% \"spark-cassandra-connector\" % \"2.0.2\").exclude(\"io.netty\", \"netty-handler\"),\n",
    "  (\"com.datastax.cassandra\" % \"cassandra-driver-core\" % \"3.0.0\").exclude(\"io.netty\", \"netty-handler\")\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f7ed9bb-fbd6-4c16-9d55-907b9cf3b5a8",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### KafkaSpark.scala\n",
    "\n",
    "In the beginning we shall make the spark session and spark context, and load the data from kafka with topic `avg`. Then we convert the initial data into a temporary table and split the key value pair, into two columns and group by the key. In the mapping function, we have key (where we defined in the `groupByKey`, values with all items in the row, Group state with sum and count (`Double` and `Int` respectively). Inside the mapping function, we get the sum and count as variables, and using `foreach` loop to compute summation of `value` column. Later we update the state with sum and count, and in the end return the key average pair as usual. Finally, we use `mapGroupsWithState` to apply the mapping function, and write the stream then finish the whole program. \n",
    "\n",
    "```scala\n",
    "package sparkstreaming\n",
    "\n",
    "import java.util.HashMap\n",
    "import org.apache.kafka.clients.consumer.ConsumerConfig\n",
    "import org.apache.kafka.common.serialization.StringDeserializer\n",
    "import org.apache.spark.streaming.kafka._\n",
    "import kafka.serializer.{DefaultDecoder, StringDecoder}\n",
    "import org.apache.spark.SparkConf\n",
    "import org.apache.spark.streaming._\n",
    "import org.apache.spark.streaming.kafka._\n",
    "import org.apache.spark.storage.StorageLevel\n",
    "import java.util.{Date, Properties}\n",
    "import org.apache.kafka.clients.producer.{KafkaProducer, ProducerRecord, ProducerConfig}\n",
    "import scala.util.Random\n",
    "\n",
    "import org.apache.spark.SparkContext\n",
    "import org.apache.spark.sql.streaming.{Trigger, GroupState, GroupStateTimeout}\n",
    "import org.apache.spark.sql.{DataFrame, Dataset, Row, SparkSession}\n",
    "import org.apache.spark.sql.functions.{col, split}\n",
    "\n",
    "object KafkaSpark {\n",
    "  def main(args: Array[String]) {\n",
    "    \n",
    "  \n",
    "    val spark:SparkSession = SparkSession.builder.appName(\"avg_spark\").master(\"local[*]\").getOrCreate()\n",
    "    val sc: SparkContext = spark.sparkContext\n",
    "    sc.setLogLevel(\"WARN\")\n",
    "    val inputTable = spark.readStream.format(\"kafka\").option(\"kafka.bootstrap.servers\", \"localhost:9092\").option(\"subscribe\", \"avg\").load()\n",
    "    \n",
    "\n",
    "    import spark.implicits._\n",
    "    val temp_table = inputTable.selectExpr(\"CAST(value AS STRING)\").as[(String)]\n",
    "        .select(split(col(\"value\"),\",\").getItem(0).as(\"key\"),\n",
    "                split(col(\"value\"),\",\").getItem(1).as(\"val\")).drop(\"value\")\n",
    "        .groupByKey(x => x.getString(0))\n",
    "\n",
    "\n",
    "def mappingFunc(key: String, values: Iterator[Row], state: GroupState[(Double, Int)]):(String, Double) = {\n",
    "        var (sum, cnt) = state.getOption.getOrElse((0.0, 0))\n",
    "\n",
    "        values.foreach{value => \n",
    "            cnt = cnt + 1\n",
    "            sum = value.getString(1).toDouble + sum\n",
    "        }\n",
    "        \n",
    "        state.update((sum, cnt))\n",
    "        (key, sum/cnt)\n",
    "    }\n",
    "\n",
    "    val resultTable = temp_table.mapGroupsWithState(mappingFunc _)\n",
    "\n",
    "    resultTable.writeStream.format(\"console\")\n",
    "      .outputMode(\"update\")\n",
    "      .trigger(Trigger.ProcessingTime(0))\n",
    "      .start()\n",
    "      .awaitTermination()\n",
    "  }\n",
    "}\n",
    "```\n",
    "\n",
    "#### **Output:**\n",
    "```bash\n",
    "-------------------------------------------\n",
    "Batch: 0\n",
    "-------------------------------------------\n",
    "+---+------------------+\n",
    "| _1|                _2|\n",
    "+---+------------------+\n",
    "|  l|13.102739726027398|\n",
    "|  x|            12.625|\n",
    "|  g| 12.16600790513834|\n",
    "|  m|12.017094017094017|\n",
    "|  f|12.245689655172415|\n",
    "|  n| 12.04564315352697|\n",
    "|  k| 12.28310502283105|\n",
    "|  v|13.460869565217392|\n",
    "|  e|12.672413793103448|\n",
    "|  o| 11.11344537815126|\n",
    "|  h|12.916666666666666|\n",
    "|  z|11.841584158415841|\n",
    "|  p|12.551020408163266|\n",
    "|  d|13.304147465437788|\n",
    "|  w|12.222222222222221|\n",
    "|  y|12.452755905511811|\n",
    "|  c|12.076923076923077|\n",
    "|  u|13.352490421455938|\n",
    "|  i|13.426724137931034|\n",
    "|  q|12.178423236514522|\n",
    "+---+------------------+\n",
    "only showing top 20 rows\n",
    "\n",
    "-------------------------------------------\n",
    "Batch: 1\n",
    "-------------------------------------------\n",
    "+---+------------------+\n",
    "| _1|                _2|\n",
    "+---+------------------+\n",
    "|  l|12.519252764010675|\n",
    "|  x|12.454621592812547|\n",
    "|  g|12.459955970545813|\n",
    "|  m|12.451525893958076|\n",
    "|  f| 12.56525436833153|\n",
    "|  n|12.514680353234207|\n",
    "|  k|12.522346797726923|\n",
    "|  v|12.450045899632803|\n",
    "|  e|12.511940413115259|\n",
    "|  o|12.471619993977718|\n",
    "|  h|12.581570537208949|\n",
    "|  z|12.589227379776647|\n",
    "|  p|12.551442198501022|\n",
    "|  d|12.557758031442242|\n",
    "|  w|12.538041416976409|\n",
    "|  y|12.448592457697853|\n",
    "|  c|12.458559315560308|\n",
    "|  u|12.527782006393668|\n",
    "|  i| 12.46321752265861|\n",
    "|  q|12.422279990824986|\n",
    "+---+------------------+\n",
    "only showing top 20 rows\n",
    "\n",
    "-------------------------------------------\n",
    "Batch: 2\n",
    "-------------------------------------------\n",
    "+---+------------------+\n",
    "| _1|                _2|\n",
    "+---+------------------+\n",
    "|  l| 12.50893371757925|\n",
    "|  x|12.484007543974442|\n",
    "|  g|12.521029745642052|\n",
    "|  m| 12.52194066749073|\n",
    "|  f|12.577554673515449|\n",
    "|  n|12.537221795855718|\n",
    "|  k|12.470072936363987|\n",
    "|  v| 12.46878853267571|\n",
    "|  e|12.462496133622023|\n",
    "|  o|12.520618162344121|\n",
    "|  h|12.580966291271093|\n",
    "|  z|12.479436685108523|\n",
    "|  p| 12.53145460810966|\n",
    "|  d|12.568201897591518|\n",
    "|  w| 12.56778882938026|\n",
    "|  y|12.505762971472334|\n",
    "|  c|12.503457868098753|\n",
    "|  u|12.540193423597678|\n",
    "|  i|12.507687012391004|\n",
    "|  q| 12.40910670837678|\n",
    "+---+------------------+\n",
    "only showing top 20 rows\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f46666d7-dde3-4219-a74f-ee253177a684",
   "metadata": {},
   "source": [
    "## **Task 3**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "31a48ad8-fb8b-4a0e-917e-9b185a35ef9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.graphx._\n",
    "import org.apache.spark.rdd.RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b56fc1ab-0913-477f-8660-088eb2e3e338",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "vertices = ParallelCollectionRDD[0] at parallelize at <console>:31\n",
       "edges = ParallelCollectionRDD[1] at parallelize at <console>:39\n",
       "graph_network = org.apache.spark.graphx.impl.GraphImpl@3bbd3f78\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "source": "user"
    },
    {
     "data": {
      "text/plain": [
       "org.apache.spark.graphx.impl.GraphImpl@3bbd3f78"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val vertices = sc.parallelize(Array((1L, (\"Alice\", 28)),\n",
    "                                    (2L, (\"Bob\", 27)), \n",
    "                                    (3L, (\"Charlie\", 65)), \n",
    "                                    (4L, (\"David\", 42)),\n",
    "                                    (5L, (\"Ed\", 55)),\n",
    "                                    (6L, (\"Fran\", 50)),\n",
    "                                    (7L, (\"Alex\", 55))))\n",
    "                                     \n",
    "val edges     = sc.parallelize(Array(\n",
    "                               Edge(2L, 1L, 7),\n",
    "                               Edge(2L, 4L, 2), \n",
    "                               Edge(3L, 6L, 3),\n",
    "                               Edge(3L, 2L, 4),\n",
    "                               Edge(4L, 1L, 1), \n",
    "                               Edge(5L, 2L, 2),\n",
    "                               Edge(5L, 3L, 8),\n",
    "                               Edge(5L, 6L, 3),\n",
    "                               Edge(7L, 5L, 3),\n",
    "                               Edge(7L, 6L, 4)))\n",
    "\n",
    "val graph_network = Graph(vertices, edges)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcf88866-6c05-41a4-ae23-42d94f9ecde8",
   "metadata": {},
   "source": [
    "### **1. Display the names of the users that are at least 30 years old.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52b4c920-d49b-4fc5-b310-91af1f571977",
   "metadata": {},
   "source": [
    "To filter our the users age, we make a filter on the 2nd attribute of the \n",
    "\n",
    "vertices >= 30 and the next step will map the names output\n",
    "```scala\n",
    "graph_network.vertices.filter(_._2._2 >= 30).collect()\n",
    "```\n",
    "Output without map names: we can see they are at least 30 years old\n",
    "```scala\n",
    "Array((4,(David,42)), (5,(Ed,55)), (6,(Fran,50)), (3,(Charlie,65)), (7,(Alex,55)))\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "35dc0fb1-76e7-4c94-a9ee-715d006ee80e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "age_30_names = Array(David, Ed, Fran, Charlie, Alex)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "source": "user"
    },
    {
     "data": {
      "text/plain": [
       "Array(David, Ed, Fran, Charlie, Alex)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val age_30_names = graph_network.vertices.filter(_._2._2 >= 30).map(_._2._1).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a05fe23-d450-46f7-b014-202a19c7c11c",
   "metadata": {},
   "source": [
    "### **2. Display who likes who.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f21013b4-ecea-4d7f-887f-27598c80f36c",
   "metadata": {},
   "source": [
    "To answer this question, we use triplets to print out two people."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2c3def60-abeb-486d-91cb-a2f5694fd8ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array(Bob likes Alice, Bob likes David, Charlie likes Bob, Charlie likes Fran, David likes Alice, Ed likes Bob, Ed likes Charlie, Ed likes Fran, Alex likes Ed, Alex likes Fran)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph_network.triplets.map(triplet =>triplet.srcAttr._1 + \" likes \" + triplet.dstAttr._1).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b1f092b-12c4-4c78-b235-33b4c581de26",
   "metadata": {},
   "source": [
    "### **3. If someone likes someone else more than 5 times than that relationship is getting pretty serious, so now display the lovers.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bb77bd9-ad09-4dad-8bc1-2e96e9a77388",
   "metadata": {},
   "source": [
    "To answer this question, we use smiliar way as the last question, namely triplets with a filter to find out who are the lovers. In the filter we filter out the attribute > 5. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c90452ed-3b57-457d-abbc-2870000745a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array(Bob is lover with Alice, Ed is lover with Charlie)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph_network.triplets.filter(_.attr>5).map(triplet =>triplet.srcAttr._1 + \" is lover with \" + triplet.dstAttr._1).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb67072c-4fdd-4b32-9630-c3c3410c53e0",
   "metadata": {},
   "source": [
    "### **4. Print the number of people who like each user (e.g., Alice is liked by 2 people).**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3365a04-73e9-482e-9301-13711e231327",
   "metadata": {},
   "source": [
    "In this question we first define a User class with indegree and outdegree. Then we map the origial graph and create the empty graph. Next, we use `outerJoinVertices` to fillin both indegree and outdegree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0f7a9c1c-f0a4-46fa-9328-772a1b179977",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defined class User\n",
       "user_graph_map = org.apache.spark.graphx.impl.GraphImpl@79c4653a\n",
       "user_graph = org.apache.spark.graphx.impl.GraphImpl@f7c1b63\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "source": "user"
    },
    {
     "data": {
      "text/plain": [
       "org.apache.spark.graphx.impl.GraphImpl@f7c1b63"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "case class User(id:Long, name: String, age: Int, indegree: Int, outdegree: Int)\n",
    "\n",
    "val user_graph_map = graph_network.mapVertices{case(id, (name,age)) => User(id, name, age, 0, 0)}\n",
    "\n",
    "val user_graph = user_graph_map.outerJoinVertices(graph_network.inDegrees){\n",
    "(id, u, indegree_value) => User(id, u.name, u.age, indegree_value.getOrElse(0), u.outdegree)\n",
    "}.outerJoinVertices(graph_network.outDegrees){\n",
    "(id, u, outdegree_value) => User(id, u.name, u.age, u.indegree, outdegree_value.getOrElse(0))\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b7a2662-9772-48a6-9e34-32c6fb8fb668",
   "metadata": {},
   "source": [
    "Also we use `for` loop to extract name and indegree then print it out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e293283a-7cfc-48fa-bde9-37d3aed77cfa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "David is liked by 1 people\n",
      "Alice is liked by 2 people\n",
      "Ed is liked by 1 people\n",
      "Fran is liked by 3 people\n",
      "Bob is liked by 2 people\n",
      "Charlie is liked by 1 people\n",
      "Alex is liked by 0 people\n"
     ]
    }
   ],
   "source": [
    "for ((name, indegree) <- user_graph.vertices.map{case(id, user) => (user.name, user.indegree)}.collect){\n",
    "    println(name  + \" is liked by \" + indegree + \" people\")\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7ff2d7f-3ccb-46b0-b614-b15f5edad204",
   "metadata": {},
   "source": [
    "### **5. Print the names of the users who are liked by the same number of people they like (e.g., Bob and David).**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1a0729a-3e77-4d18-9c4a-9849d6c561f3",
   "metadata": {},
   "source": [
    "To answer this question, we use the graph in the last question, if the `indegree` and `outdegree` are equal we print out the names of them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "77c6e291-bc62-42da-aaa1-fd4bb0fd7cdd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "David\n",
      "Bob\n"
     ]
    }
   ],
   "source": [
    "for ((name, indegree, outdegree) <- user_graph.vertices.map{case(id, user) => (user.name, user.indegree, user.outdegree)}.collect){\n",
    "    if (indegree == outdegree) {\n",
    "        println(name)\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c532a4c-9b66-46fa-a10d-2bb0354d0f09",
   "metadata": {},
   "source": [
    "### **6. Find the oldest follower of each user (hint: use the aggregateMessages).**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b713a7ca-d3be-4d67-89ac-df50980c1ea1",
   "metadata": {},
   "source": [
    "In this question we can use `aggregateMessages`, send the messages to triplet of names and ages. In the end we leave the larger message."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "292e15b3-03b8-454a-a27a-02e50a32df57",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "follower_rdd = VertexRDDImpl[62] at RDD at VertexRDD.scala:57\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "source": "user"
    },
    {
     "data": {
      "text/plain": [
       "VertexRDDImpl[62] at RDD at VertexRDD.scala:57"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val follower_rdd: VertexRDD[(String, Int)] = user_graph.aggregateMessages[(String, Int)](\n",
    "    // map\n",
    "    triplet => {\n",
    "        triplet.sendToDst(triplet.srcAttr.name, triplet.srcAttr.age)\n",
    "    },\n",
    "    // reduce if message is larger then take the larger one\n",
    "    (a, b) => (if (a._2 > b._2) {\n",
    "                    a\n",
    "    } else {\n",
    "                    b\n",
    "    }\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "210e04f6-fb97-45e7-8e46-f88f3738e76a",
   "metadata": {},
   "source": [
    "Then, we use a `for` loop to print out all the follower. if the user doesn't have any follower then we print out there have no followers. If the user has follower then print out the name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ec7fca90-36dc-44b3-b30d-3cea298a5d7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bob is the oldest follower of David\n",
      "David is the oldest follower of Alice\n",
      "Alex is the oldest follower of Ed\n",
      "Charlie is the oldest follower of Fran\n",
      "Charlie is the oldest follower of Bob\n",
      "Ed is the oldest follower of Charlie\n",
      "Alex doesn't have follower!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ug_oldeest = VertexRDDImpl[64] at RDD at VertexRDD.scala:57\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "source": "user"
    },
    {
     "data": {
      "text/plain": [
       "VertexRDDImpl[64] at RDD at VertexRDD.scala:57"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val ug_oldeest = user_graph.vertices.leftJoin(follower_rdd) {\n",
    "    case (id, user, follower_list) => (user.name, follower_list)\n",
    "}\n",
    "\n",
    "for ((name, follower) <- ug_oldeest.map(follower_list => (follower_list._2._1, (follower_list._2._2))).collect){\n",
    "    if (follower == None){\n",
    "        println(name + \" doesn't have follower!\")\n",
    "    } else {\n",
    "   \n",
    "    println(follower.map(_._1).get +\" is the oldest follower of \"+ name)\n",
    "    }\n",
    "}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Apache Toree - Scala",
   "language": "scala",
   "name": "apache_toree_scala"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "2.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
